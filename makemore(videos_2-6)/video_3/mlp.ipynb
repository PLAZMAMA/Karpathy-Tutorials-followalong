{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from random import shuffle\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import torch.nn.functional as tfunc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "torch.set_default_device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open(\"../names.txt\", \"r\").read().splitlines()\n",
    "chars = sorted(list(set(\"\".join(words))))\n",
    "char_to_indx = {char: indx+1 for indx, char in enumerate(chars)}\n",
    "char_to_indx[\".\"] = 0\n",
    "indx_to_char = {indx: char for char, indx in char_to_indx.items()}\n",
    "\n",
    "def train_dev_test_split(words, train_percentage, dev_percentage):\n",
    "    shuffled_words = deepcopy(words)\n",
    "    shuffle(shuffled_words)\n",
    "    train_dev_split_point = math.ceil(len(words) * train_percentage)\n",
    "    dev_val_split_point = math.floor(\n",
    "        train_dev_split_point + (len(words) * dev_percentage)\n",
    "    )\n",
    "    return (\n",
    "        shuffled_words[:train_dev_split_point],\n",
    "        shuffled_words[train_dev_split_point:dev_val_split_point],\n",
    "        shuffled_words[dev_val_split_point:],\n",
    "    )\n",
    "\n",
    "def create_dataset(words, char_to_indx, block_size = 3):\n",
    "    inputs, labels = [], []\n",
    "    for word in words:\n",
    "        # print(word)\n",
    "        context = [0] * block_size\n",
    "        for label in word + \".\":\n",
    "            label_indx = char_to_indx[label]\n",
    "            inputs.append(context)\n",
    "            labels.append(label_indx)\n",
    "            # print(\"\".join(indx_to_char[indx] for indx in context), \"------>\", label)\n",
    "            context = context[1:] + [label_indx]\n",
    "        # print()\n",
    "\n",
    "    return torch.tensor(inputs), torch.tensor(labels)\n",
    "\n",
    "BLOCK_SIZE = 5\n",
    "train_words, dev_words, test_words = train_dev_test_split(words, 0.8, 0.1)\n",
    "train_inputs, train_labels = create_dataset(train_words, char_to_indx, block_size=BLOCK_SIZE)\n",
    "dev_inputs, dev_labels = create_dataset(dev_words, char_to_indx, block_size=BLOCK_SIZE)\n",
    "test_inputs, test_labels = create_dataset(test_words, char_to_indx, block_size=BLOCK_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing Model Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL PARAMS: 32567\n"
     ]
    }
   ],
   "source": [
    "# Params\n",
    "gen = torch.Generator(device=\"cuda\").manual_seed(2147483647)\n",
    "CHAR_FEATURES_NUM = 20\n",
    "HIDDEN_UNITS_NUM = 250\n",
    "C = torch.randn((27, CHAR_FEATURES_NUM), generator=gen, requires_grad=True) # Lookup table\n",
    "weights1 = torch.randn((BLOCK_SIZE * CHAR_FEATURES_NUM, HIDDEN_UNITS_NUM), generator=gen, requires_grad=True)\n",
    "bias1 = torch.randn(HIDDEN_UNITS_NUM, generator=gen, requires_grad=True)\n",
    "weights2 = torch.randn((HIDDEN_UNITS_NUM, 27), generator=gen, requires_grad=True)\n",
    "bias2 = torch.randn(27, generator=gen, requires_grad=True)\n",
    "parameters = [C, weights1, bias1, weights2, bias2]\n",
    "print(f\"TOTAL PARAMS: {sum([param.nelement() for param in parameters])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 200000\n",
    "MINI_BATCH_SIZE = 50\n",
    "\n",
    "for indx in range(0, EPOCHS):\n",
    "    mini_batch_indxs = torch.randint(0, train_inputs.shape[0], (MINI_BATCH_SIZE,))\n",
    "    mini_batch_inputs = train_inputs[mini_batch_indxs]\n",
    "    mini_batch_labels = train_labels[mini_batch_indxs]\n",
    "\n",
    "    embed = C[mini_batch_inputs]\n",
    "    joined_embed = embed.view(embed.shape[0], embed.shape[1] * embed.shape[2])\n",
    "    layer1_out = torch.tanh(joined_embed @ weights1 + bias1)\n",
    "    logits = layer1_out @ weights2 + bias2\n",
    "    loss = tfunc.cross_entropy(logits, mini_batch_labels)\n",
    "\n",
    "    for param in parameters:\n",
    "        param.grad = None\n",
    "\n",
    "    learning_rate = 0.1 if indx < EPOCHS // 2 else 0.01\n",
    "    loss.backward()\n",
    "    for param in parameters:\n",
    "        param.data += learning_rate * -param.grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval Model with Dev Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[132], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m logits \u001b[38;5;241m=\u001b[39m layer1_out \u001b[38;5;241m@\u001b[39m weights2 \u001b[38;5;241m+\u001b[39m bias2\n\u001b[1;32m      5\u001b[0m loss \u001b[38;5;241m=\u001b[39m tfunc\u001b[38;5;241m.\u001b[39mcross_entropy(logits, train_labels)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m embed \u001b[38;5;241m=\u001b[39m C[dev_inputs]\n\u001b[1;32m      9\u001b[0m joined_embed \u001b[38;5;241m=\u001b[39m embed\u001b[38;5;241m.\u001b[39mview(embed\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], embed\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m embed\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m])\n",
      "File \u001b[0;32m~/src/Karpathy-Tutorials-followalong/.venv/lib/python3.11/site-packages/torch/utils/_device.py:104\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    103\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m--> 104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "embed = C[train_inputs]\n",
    "joined_embed = embed.view(embed.shape[0], embed.shape[1] * embed.shape[2])\n",
    "layer1_out = torch.tanh(joined_embed @ weights1 + bias1)\n",
    "logits = layer1_out @ weights2 + bias2\n",
    "loss = tfunc.cross_entropy(logits, train_labels)\n",
    "print(f\"Train loss: {loss.item()}\")\n",
    "\n",
    "embed = C[dev_inputs]\n",
    "joined_embed = embed.view(embed.shape[0], embed.shape[1] * embed.shape[2])\n",
    "layer1_out = torch.tanh(joined_embed @ weights1 + bias1)\n",
    "logits = layer1_out @ weights2 + bias2\n",
    "loss = tfunc.cross_entropy(logits, dev_labels)\n",
    "print(f\"Dev loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best dev loss: 2.1682236194610596"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
